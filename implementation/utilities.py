import pickle
import nltk
import sys
import string
import features

maxTrainingIterations = 300

# Function to parse context information from a context file
# parameters:
#	filename: The name of the file to parse from
# returns: an array of feature function result sets tied to correct answers for that case
def setupData(filename):
	# open the file
	f = open(filename, 'r')

	# read the raw context data
	lines = f.readlines()

	# intialize the array
	contexts = []

	# split the contexts into token lists
	for line in lines:
		words = (string.split(line))
		contexts.append(words)

	# set up the features array
	contextFeatures = []

	# set up data for each context
	for context in contexts:
		contextCase = []
		contextCase.append(context)
		# initialize the array.
		case = []

		# calculate the feature functions
		feats = features.testFeatures(context)

		# parse the "correctness" information from the data
		if context[7] == "<Y>":
			classification = "y"
		else:
			classification = "n"

		# tie it all together nicely
		case.append(feats)
		case.append(classification)
		contextCase.append(case)
		contextFeatures.append(contextCase)

	# return the data
	return contextFeatures

# Function to print out a context
def printContext(context):
	# add the prefix
	string = "prefix:"
	for i in range(0,3):
		word = context[i]
		string += " '" + word + "'"

	# add the candidate
	string += " candidate:"
	string += " '" + context[3] + "'"

	# add the suffix
	string += " suffix:"
	for i in range(4,7):
		word = context[i]
		string += " '" + word + "'"

	# add the classification
	string += " class: "
	string += context[7]

	# return the string
	return string


def printFeatures(features):
	string = ""
	for feature in features.keys():
		string += feature + "=" + str(features[feature]) +"\n"

	return string

# Function to train a model
# parameters:
#	algorithm: the nltk training algorithm to use
#	trainingData: a data package generated by setupData
# returns: a MaxentClassifier object representing the maximum-entropy model generated, or an Exception if one is to be had.
def train_maxent( trainingData):

	#pull out just the feature functions
	parsedData = []
	for context in trainingData:
		parsedData.append(context[1])

	try:
		classifier = nltk.classify.maxent.MaxentClassifier.train(parsedData, 'GIS', trace=0, max_iter=maxTrainingIterations)
	except Exception, e:
		classifier = e
	return classifier

# Function to classify data using a given MaxentClassifier
def classify_maxent(model, data):

	# handle erroneous models
	if isinstance(model, Exception):
		print "Invalid Model"

	# proceed if it's not an exception
	else:

		#initialize summary statistics
		numTests = 0
		truePositives = 0
		trueNegatives = 0
		falsePositives = 0
		falseNegatives = 0

		# for each case
		for case in data:
			featureData = case[1]
			# get the features
			featureset = featureData[0]

			# classify the data and calculate distributions
			pdist = model.prob_classify(featureset)
			label = model.classify(featureset)

			# grab the correct value for this case
			correct = featureData[1]

			#check for errors
			if correct != label:
				# print the test label
				print "Test #%d:" % (numTests),
				print "\t",

				# report all the data
				print 'n: %.2f y: %.2f descision: %s, correct: %s' % (pdist.prob('n'), pdist.prob('y'), label, correct),

				if label == 'y':
					print " false positive\n\tContext:\n " + printContext(case[0])
					print "\tFeatures:\n " + printFeatures(featureData[0])
					falsePositives += 1
				else:
					falseNegatives += 1
					print " false negative\n\tContext:\n" + printContext(case[0])
					print "\tFeatures:\n " + printFeatures(featureData[0])
			else:
				if label == 'y':
					truePositives += 1
				else:
					trueNegatives += 1

			numTests += 1

		# calculate summary statistics
		accuracy = float(truePositives + trueNegatives) / float(numTests)
		precision = float(truePositives) / float(truePositives + falsePositives)
		recall = float(truePositives) / float(truePositives + falseNegatives)

		# print out summary statistics
		print
		print "Summary Statictics:"
		print 'number of tests: %i' % (numTests)
		print 'true positives:  %i' % (truePositives)
		print 'true negatives:  %i' % (trueNegatives)
		print 'false positives: %i' % (falsePositives)
		print 'false negatives: %i' % (falseNegatives)
		print 'accuracy:        %.3f' % (accuracy)
		print 'precision:       %.3f' % (precision)
		print 'recall:          %.3f' % (recall)

def execute(model, data):
    # handle erroneous models
    if isinstance(model, Exception):
        print "Invalid model."
        return Exception
    # proceed if it's not an exception
    else:
        # initialize the classification list
        classificationList = []

        # classify each case
        for case in data:
            featureData = case[1]
            featureSet = featureData[0]
            classification = model.classify(featureset)

            # and add it's classification to the list
            classificationList.append(classification)

        # return the data
        return classificationList