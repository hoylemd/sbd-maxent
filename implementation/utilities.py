import pickle
import nltk
import sys
import string
import features

# Function to parse context information from a context file
# parameters: 
#	filename: The name of the file to parse from
# returns: an array of feature function result sets tied to correct answers for that case
def setupData(filename):
	# open the file
	f = open(filename, 'r')

	# read the raw context data
	lines = f.readlines()
	
	# intialize the array
	contexts = []

	# split the contexts into token lists 
	for line in lines:
		words = (string.split(line))
		contexts.append(words)

	# set up the features array
	contextFeatures = []	

	# set up data for each context
	for context in contexts:

		# initialize the array.
		case = []
		
		# calculate the feature functions
		feats = features.testFeatures(context)

		# parse the "correctness" information from the data
		if context[7] == "<Y>":
			classification = "y"
		else:
			classification = "n"

		# tie it all together nicely
		case.append(feats)
		case.append(classification)
		contextFeatures.append(case)

	# return the data
	return contextFeatures


# Function to train a model
# parameters: 
#	algorithm: the nltk training algorithm to use
#	trainingData: a data package generated by setupData
# returns: a MaxentClassifier object representing the maximum-entropy model generated, or an Exception if one is to be had.
def train_maxent( trainingData):
	try:
		classifier = nltk.MaxentClassifier.train(trainingData, 'GIS', trace=0, max_iter=1000)
	except Exception, e:
		classifier = e	
	return classifier

# Function to classify data using a given MaxentClassifier
def classify_maxent(model, data):
	
	# handle erroneous models
	if isinstance(model, Exception):
		print 'Error: %r' % model
	
	# proceed if it's not an exception
	else:
		i = 1

		#initialize summary statistics
		numTests = 0
		truePositives = 0
		trueNegatives = 0
		falsePositives = 0
		falseNegatives = 0

		# for each case
		for case in data:
				
			# get the features
			featureset = case[0]

			# print the test label
			print "Test #%d:" % (i),
			i += 1
			print "\t",

			# classify the data and calculate distributions
			pdist = model.prob_classify(featureset)
			label = model.classify(featureset)

			# grab the correct value for this case
			correct = case[1]

			#report all the data
			print 'n: %.2f y: %.2f descision: %s, correct: %s' % (pdist.prob('n'), pdist.prob('y'), label, correct),

			#check for errors
			if correct != label:
				if label == 'y':
					print " false positive"
					falsePositives += 1
				else:
					print " false negative"
					falseNegatives += 1
			else:
				print
				if label == 'y':
					truePositives += 1
				else:
					trueNegatives += 1

			numTests += 1

		# calculate summary statistics
		accuracy = float(truePositives + trueNegatives) / float(numTests)
		precision = float(truePositives) / float(truePositives + falsePositives)
		recall = float(truePositives) / float(truePositives + falseNegatives)		

		# print out summary statistics
		print
		print "Summary Statictics:"
		print 'number of tests: %i' % (numTests)
		print 'true positives:  %i' % (truePositives)
		print 'true negatives:  %i' % (trueNegatives)
		print 'false positives: %i' % (falsePositives)
		print 'false negatives: %i' % (falseNegatives)
		print 'accuracy:        %.3f' % (accuracy)
		print 'precision:       %.3f' % (precision)
		print 'recall:          %.3f' % (recall) 
